Making a Case for 3D Convolutions for Object Segmentation in Videos

用于分割视频中的显着对象的一些现有方法（例如[16、36]）遵循[34]的方法，并在执行学习的两种信息流融合之前，首先分别处理外观和运动信息。

另一方面，对于视频动作分类的任务，有几种方法[10、12、38、39]将视频建模为3D体积，并利用3D卷积网络共同学习空间和时间特征，我们认为这是向视频动作分类迈出的一步。正确的方向。

但是，将3D CNN用于像素精确的分割任务会带来一些挑战。

首先，与具有相同架构和深度的2D同类网络相比，这些网络通常速度较慢，并且包含的可训练参数明显更多。对于需要比用于分类任务的图像分辨率更高的图像分辨率的分割任务，这尤其成问题。

其次，对于分割任务而言，重要的是（参见[4]），拥有一种网络架构，该架构可以针对每个图像像素捕获较大的接收场并有效利用多尺度特征信息。

据我们所知，Hou等人的工作 [15]是将完整的3D CNN应用于视频对象分割的首次尝试。 尽管很有希望，但它却不如使用2D CNN的最新技术[47，57]好。 这主要是由于两个原因。

首先，为了保持计算负载的可管理性，他们采用了浅3D ResNet-34 [38]作为编码器网络。 其次，他们遵循通常采用的设计选择，即在backbone中使用较小的步幅来保留特征定位，并另外使用无规则的卷积来维持较大的接收场[4，5]。

结果，他们的方法在网络的整个深度上传播了大型特征图，这显着增加了训练和预测期间的内存占用量和运行时间。

-------------
在本文中，我们提出了一种缓解上述问题并优于[15]以及现有基于2D CNN的现有技术的网络架构[16、36、47、57]。我们认为将3D CNN应用于此类任务的更好方法涉及使用标称跨度的轻量级编码器网络。
这样做可以释放计算预算，然后可以将其更好地用于增强解码器。
特别是，我们使用在大型视频动作分类数据集上经过预训练的，计算效率高的通道分隔网络[39]作为编码器。在解码器中，我们使用了全局卷积[29]和优化模块[30，51]的新颖3D变体。这些
使我们能够捕获大的接收场并分别从多尺度编码器功能中学习高质量的分割蒙版。

为了验证我们网络的有效性，我们将其应用于与视频中显着对象分割相关的三个数据集基准：
DAVIS’16无监督[32]，弗赖堡-伯克利运动分割（FBMS）[27]和ViSal [49]。我们证明了我们的网络不仅比现有的最新技术要快，而且在性能上也大大超过了它们。此外，我们执行了一些消融实验，定量地证明了我们各种设计选择的合理性。

总而言之，在本文中，我们
（i）证明3D CNN可以大大胜过涉及像素精确视频分割的任务的现有方法（基于2D CNN）；
（ii）提出了全局卷积[29]和优化模块[30，51]的新颖3D变体，这些变体显着提高了解码器的性能；
（iii）在三个数据集上建立新的最新技术。 我们相信我们的结果将激励其他人将相似的网络体系结构用于涉及像素精确视频理解的其他任务，例如发现新颖的对象类别[21、28、44、52]，半监督视频对象分割[2]， 视频实例分割[56]和多对象跟踪与分割[43]。

-Related Work------------

用于视频动作分类的3D CNN早期工作[17，18，37，41]将3D CNN应用于视频人类动作分类，使用的是浅层的，通常是自定义的网络架构，类似于当时的2D CNN对应部分。为了克服注释视频数据的不足，[3，8]提出了利用2D图像数据来训练3D CNN的新颖方法。

后来，随着更大的视频数据集的出现（例如[19]），有可能从头开始有效地训练深层3D CNN。 [12]通过将3x3卷积内核扩展为3x3x3，将ResNet [13]体系结构扩展到了3D。但是，这样做的确非常重要
增加了计算开销。 [53]提出了将2D和3D卷积混合以提高速度和性能的方法，而[38]提出了将3D卷积分解为空间和时间卷积的R（2 + 1）D卷积。受具有通道分离卷积的2D CNN成功的启发[6]，[39]提出了一种3D通道分离的ResNet，与现有网络相比，该网络性能更好，参数更少。
[10]通过对大型视频数据进行弱监督的预训练，提高了3D CNN性能。我们表明，这样的预训练对密集的像素精确分割任务也很有帮助。

无监督视频对象分割
无监督视频对象分割的任务是为视频剪辑中表现出主要运动的对象估计二进制分割掩码。在执行两者的学习融合之前，FusionSeg [16]和LVO [36]在单独的流中处理光流和外观。 [58]使用超像素生成每帧对象建议，并将它们与时间相关联，随后进行滤波以获得主导对象。与这些作品不同，我们的3D CNN方法本质上是学会以统一的方式推理外观和动作。本着[35]的精神，使用卷积LSTM [54]来利用视频数据的顺序性质并共同学习时空特征。但是，他们在顶部使用基于CRF的模型来获取二进制分割掩码。

通常，采用光流，对象提议关联或RNN的方法都难以建立远程连接。 为了解决这个问题，AD-Net [57]学习将参考图像帧的区域与任意查询帧中的区域相关联。 但是，这种方法不能有效地利用几个框架中的上下文。 AGNN [47]使用图形神经网络在帧之间传递消息，以便对远程时间连接建模。 STEm-Seg [1]使用类似编解码器的架构，在解码器中进行3D卷积，以学习时间上下文。 但是，他们的编码器网络是完全2D的。 [15]与我们的方法最相似，因为它提出了一个完整的3D编码器/解码器网络，但是，我们提出的网络体系结构与它们的体系结构有所不同，并且实现了更高的性能。

Video Salient Object Detection
其他几项工作使用涉及视频显着目标检测的各种命名法解决了相同的问题。 基于非深度学习的方法[9、26、45、49]通常使用手工制作的功能来创建单独的帧内和帧间显着性图。 将这些图合并为一个连贯的序列的任务
然后将分割蒙版公式化为优化问题。 [22]通过与LSTM结合使用基于光流的运动提示来学习显着性模型，而[50]使用CNN来学习单帧显着性，然后应用动态显着性模型来处理时间连接。 与所有这些作品不同，我们使用3D CNN共同学习时空域上的显着性模型。

===Method===============
我们在视频中分割显着对象区域的方法基于一种编码器-解码器体系结构，该体系结构利用3D卷积来共同学习时空特征。

如第1节所述，像素精度细分任务得益于更高的图像分辨率和具有大接收域的网络，这在处理3D CNN时在计算上具有挑战性。 我们通过采用有效的通道分隔编码器网络[39]和解码器来缓解这些挑战，该解码器包括（i）可以捕获大接收场的新型3D全球卷积（GC3D），以及（ii）有效地优化的新型3D refinement模块 将多尺度编码器功能集成到高质量mask中。

3.1 Encoder
我们网络的编码器是一种计算效率高的3D ResNet，具有通道分隔的卷积，已成功用于视频动作分类[39]。 
特别是，我们使用其模型的简化交互（ir-CSN）变体，其中将ResNet瓶颈块中的每个3x3x3卷积替换为3x3x3深度可分离卷积，而瓶颈中已有的1x1x1卷积 捕获渠道互动。

这种架构减少的内存占用量，结合我们建议的解码器架构（第3.2节），可以将该网络的152层变体切实可行地应用于像素精确的分割任务。

为了证明该设计决策的合理性，我们对表1的近期工作中使用的各种主干的计算开销进行了定量分析。尽管具有更深的主干，但与其他较浅的网络相比，基于ResNet-152的ir-CSN的参数明显更少。

就运行时间而言，只有普通的2D ResNet-101稍快一些，但是，此类2D网络本质上无法学习时间上下文。

最先进的方法[47，57]要么使用诸如DeepLabV3的[4] ResNet-101主干之类的2D网络，要么使用具有无穷卷积和减小的步幅的浅3D网络[15]。

尽管此策略提高了分段任务的性能，但主要缺点是它还显着增加了内存占用量和运行时间。

我们认为，更好的方法是使用计算效率高的通道分隔主干，且标称步幅大。 这不仅使我们拥有更深的基础，通常可以学习更好的最终任务功能，而且更重要的是，它释放了宝贵的计算预算，这些预算可用于增强解码器的效能。

3.2 Decoder

对于输入视频剪辑，编码器会生成4种不同比例的特征图。 解码器体系结构包括一系列3D卷积和上采样层，这些层将这些特征图精炼成最终的分割掩码。

为了在编码器特征中捕获大的接收场，Chen等[5] 提出使用具有减小的步幅（8x或16x）的编码器与Atrous空间金字塔池（ASPP）模块结合使用，该模块将具有不同膨胀率的多个并行Atrous卷积应用于特征图。 [15]还提出了ASPP的3D变体，并在其网络中使用了它。

相比之下，我们以通常用于分类任务的32x标称步幅学习编码器功能。

为了捕获广阔的空间环境，我们提出了Global卷积网络的3D变体，该变体被引入用于图像中的语义分割[29]。

这里的想法是，可以用内核大小分别为1×k和k×1的一系列行和列卷积代替大的k×k卷积。 这产生相同的有效接收场，同时具有较少的参数。 我们的3D Global 卷积模块（GC3D）包含3D卷积，其沿时间维度的内核大小为统一。 这是因为输入视频剪辑的时间维度通常比空间维度小得多。

为了组合和上采样多尺度特征图，我们另外提出了[30]中引入的Refinement模块的3D变体，用于生成图像中的对象建议。

这里的基本思想是通过跳过连接将两个3x3x3卷积应用于给定的特征图，然后进行三线性上采样并在该比例下与相应的编码器特征图相加。

接下来是带有跳过连接的两个进一步的卷积。 图2说明了GC3D和3D优化模块，图3说明了整个网络体系结构。
![image](https://user-images.githubusercontent.com/11287531/116231940-9b342a00-a7ad-11eb-825c-a2444fcdbf5d.png)

![image](https://user-images.githubusercontent.com/11287531/116231465-08938b00-a7ad-11eb-925f-d1070d4c7801.png)



3.3 Video Clip Sampling

为了获得最佳的网络性能，训练和推理之间输入片段的时间长度应保持一致。

因此，为了将网络应用于任意长度的视频，我们将输入视频划分为长度为Tc的片段，连续片段之间的To重叠。

对于重叠的帧，将掩模概率平均以产生最终的分割掩模。

通常，我们的方法因此是近乎online的，因为给定一个新的帧，在最多Tc -To -1个时间步长之后（视频流中最开始的Tc帧除外），它的分段掩码才可用。 
请注意，如果满足以下条件T<sub>o</sub><-T<sub>c</sub>-1，则可以实现online变体

