Making a Case for 3D Convolutions for Object Segmentation in Videos

**1 Introduction**

关于分割视频里显眼的(salient)物体， 一些方法[16, 36]借鉴了[34]的方法， 在学习融合两种信息之前，先分别处理外观和移动信息。

关于视频动作分类，一些方法[10、12、38、39]将视频建模为3D volumes 然后运用3D CNN联合学习空间和时间特征。作者觉得这个方向是对的。

但是，将3D CNN用于像素精确的分割任务会带来一些挑战。

1. 与具有相同架构和深度的2D CNN相比，这些网络通常速度较慢，并且需要训练多很多的参数。对于分割任务，尤其是问题，因为分割任务比分类任务要求更高图像分辨率。

2. 对于分割任务，重要的是[4]，拥有一种网络架构，该架构可以针对各个图像像素捕获较大的感受野 并有效利用多尺度特征信息*What?*。

Hou等人[15]首先将完整的3D CNN应用于视频对象分割。 尽管很不错，但它却不如使用2D CNN的方法[47，57]表现好。 两个原因：

首先，为了保持计算负载的可管理性，他们采用了较浅的3D ResNet-34 [38]作为编码器网络。 其次，他们按常理在backbone中使用较小的stride来保留特征定位，并使用astrous卷积来维持较大的感受野[4，5]。

结果，他们的方法在网络的整个深度上传播了大型特征图，这显著增加了训练和预测期间的内存占用量和运行时间。

在本文中，我们提出了一种网络架构，优于Hou[15]以及现有基于2D CNN的现有技术的网络架构[16、36、47、57]。我们认为将3D CNN应用于此类任务的更好方法是 使用正常stride的轻量编码器网络。这样做计算不累，留劲去增强解码器。具体说，我们使用在 大型视频动作分类 数据集上经过预训练的，一个计算效率高的channel-seperated网络[39]作为编码器。在解码器中，我们使用了一些全局卷积[29]新颖的3D变体 和 一些优化模块[30，51]。这些使我们能够捕获大的感受野 并能 从多种尺寸的来自于编码器的feature学习到高质量的mask 。

为了验证我们网络的有效性，我们将其应用于与视频中显着对象分割相关的三个数据集基准：DAVIS’16无监督[32]，FBMS[27]和ViSal [49]。
我们证明了我们的网络不仅比现有的最新技术要快，而且在性能上也大大超过了它们。此外，我们执行了一些消融实验，定量地证明了我们各种设计选择的合理性。

总之，我们
（i）证明3D CNN可以大大胜过 现有基于2D CNN的精确像素视频分割方法；
（ii）提出了 全局卷积[29]的新颖3D变体和优化模块[30，51]，这些明显提高了解码器的性能；
（iii）在三个数据集上表现更好。 

-------------
**2 Related Work**

3D CNNs for Video Action Classification

早期工作[17，18，37，41]将3D CNN应用于视频人类动作分类，使用的是浅层的，通常是自定义的网络架构，类似于当时的2D CNN。为了克服标注视频数据的不足，[3，8]提出了利用2D图像数据来训练3D CNN。

后来，更大的视频数据集（例如[19]），可以从头开始有效地训练深层3D CNN。 [12]通过将3x3 Kernal 扩展为3x3x3，将ResNet [13]结构扩展到了3D。但这样显著增加了计算开销。 

[53]提出了将2D和3D卷积混合以提高速度和性能的方法，而[38]提出了将3D卷积 分解为空间和时间卷积的R（2 + 1）D卷积。

受具有channel-seperated卷积的2D CNN成功的启发[6]，[39]提出了一种3D channel-seperated的ResNet，与现有网络相比，该网络性能更好，参数更少。

[10]通过对大型视频数据进行弱监督的预训练，提高了3D CNN性能。我们表明，这样的预训练对密集的像素精确分割任务也很有帮助。

Unsupervised Video-Object Segmentation

其任务是为视频剪辑中表现出主要运动的对象估计二进制分割掩码。

FusionSeg [16]和LVO [36]先在分开的流中处理光流和外观，然后学习融合他俩。

[58]使用超像素 对每帧生成object proposal，并将随时间关联，随后进行filter以获得主要对象。

与这些作品不同，我们的3D CNN方法本质上是学会以统一的方式推理外观和移动。和[35]的精神类似，使用卷积LSTM[54]来发挥视频数据的顺序性质 并联合学习时空feature。但是，[35]在顶部使用基于CRF的模型来获取mask。

通常，采用光流，object proposal关联，或RNN的方法都难以建立远程连接。 为了解决这个问题，AD-Net [57]学习 如何关联 一个参考帧的区域 和 任意查询帧的区域。但是，这种方法不能有效地利用跨帧的上下文。 

AGNN [47]使用GNN图形神经网络在帧之间传递消息，以便有更大时间跨度的信息。 
STEm-Seg [1]使用类似编解码器的架构，在解码器中进行3D卷积，以学习时间上下文。但是，他们的编码器网络是完全2D的。
[15]与我们的方法最相似，因为它提出了一个完整的3D编码器/解码器网络，但是，我们提出的网络体系结构与它们的体系结构有所不同，并且实现了更高的性能。


Video Salient Object Detection

基于非深度学习的方法[9、26、45、49]通常使用手动的功能来创建单独的帧内和帧间saliency maps。将这些图合并为一个连贯的序列的mask的任务 成为 一个 优化 问题。 
[22]通过与LSTM结合使用 基于光流的运动提示 来学习saliency模型。
而[50]使用CNN来学习单帧saliency，然后应用动态saliency模型来处理时间连接。 
与所有这些作品不同，我们使用3D CNN共同学习时空域上的saliency模型。

-------------
**3 Method**

我们 分割视频中显眼对象区域 的方法 基于一种编码器-解码器体系结构，该结构利用3D CNN来共同学习时空特征。

像素分割任务得益于更高的图像分辨率和具有大感受野的网络，这在处理3D CNN时计算量很大。 解决的方法是 采用有效的通道分隔编码器网络[39] 和 一个解码器包括（i）可以捕获大感受野的新型3D Global Convolutions（GC3D），和（ii）能有效优化多尺度编码器feature并生成高质量mask的新型3D refinement模块。

**3.1 Encoder**
编码器：一个计算效率高的3D ResNet，具有通道分隔的卷积，已成功用于视频动作分类[39]。 

具体说，使用他们的ir-CSN变体，将其中ResNet bottleneck中的每个3x3x3卷积 替换为 3x3x3 深度可分离的卷积，而bottleneck中已有的1x1x1卷积 捕获channel interactions*what？*。
这种架构内存占用量更少，结合我们的解码器），可以将该网络的152层变体 切实可行地应用于 像素分割任务。

![image](https://user-images.githubusercontent.com/11287531/116397884-3e9f4080-a87b-11eb-8518-670d17f16d50.png)

为了证明该设计决策的合理性，我们对表1的近期工作中使用的各种主干的计算开销进行了定量分析。尽管具有更深的主干，但与其他较浅的网络相比，基于ResNet-152的ir-CSN的参数明显更少。
就运行时间而言，只有普通的2D ResNet-101稍快一些，但是，此类2D网络本质上无法学习时间上下文。
最先进的方法[47，57]要么使用诸如DeepLabV3的[4] ResNet-101主干之类的2D网络，要么使用atrous卷积和减小stride的浅3D网络[15]。尽管此策略提高了分段任务的性能，但主要缺点是它还显著增加了内存占用量和运行时间。
我们认为，更好的方法是使用计算效率高的channel-separated 普通stride的 backbone。 这不仅使我们拥有更深的通常可以学习更好的最终任务的backbone，而且更重要的是，它可以把力省给解码器。

**3.2 Decoder**

对于输入clip，编码器生成4种不同比例的feature maps。 解码器 包括一系列3D卷积和上采样层，这些层将这些特征图精炼成最终的mask。

为了在编码器feature中捕获大的感受野，Chen等[5] 提出使用具有减小stride（8x或16x）的编码器 与 Atrous Spatial Pyramid Pooling（ASPP）模块结合使用。该模块 将具有不同扩张率的多个并行Atrous卷积应用于一个feature map。[15]还提出了ASPP的3D变体，并在其网络中使用了它。

相比之下，我们以通常用于分类任务的32x标准stride学习编码器feature。

为了捕获广阔的空间上下文，我们提出了Global卷积网络的3D变体，该变体被[29]引入用于图像中的语义分割。

我们的想法是，可以用 内核大小分别为1×k和k×1的一系列 行卷积和列卷积 代替 大的k×k卷积。 这能产生相同的有效感受野，同时具有较少的参数。 
我们的3D Global 卷积模块（GC3D）包含3D卷积，其沿时间维度的kernel大小统一。 这是因为输入视频剪辑的时间维度通常比空间维度小得多。

为了组合和上采样 多尺度特征图，我们另外提出了[30]中引入的Refinement模块的3D变体，用于生成图像中的object proposals。

基本思想是 将两个3x3x3卷积 应用于给定的特征图 with a skip connection *what*，然后进行三线性上采样, 并在该比例下与相应的编码器特征图相加。

接下来是 带有a skip connection的2个的卷积。 图2说明了GC3D和3D优化模块，图3说明了整个网络体系结构。
![image](https://user-images.githubusercontent.com/11287531/116231940-9b342a00-a7ad-11eb-825c-a2444fcdbf5d.png)

![image](https://user-images.githubusercontent.com/11287531/116231465-08938b00-a7ad-11eb-925f-d1070d4c7801.png)



**3.3 Video Clip Sampling**

为了获得最佳的网络性能，训练和测试时输入clip的时长应一致。因此，为了将网络应用于任意长度的视频，我们将输入视频 划分为长度为T<sub>c</sub>的片段，俩片段之间有T<sub>o</sub>重叠。对于重叠的帧，mask是以平均方式以产生。

通常，我们的方法因此是近乎online的，因为给定一个新的帧，在最多T<sub>c</sub> - T<sub>o</sub> -1个时间步长之后（视频流中最开始的T<sub>c</sub>帧除外），它的mask才可用。 
如果满足以下条件T<sub>o</sub><-T<sub>c</sub>-1，则可以实现online变体*what?*

